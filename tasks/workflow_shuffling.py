"""
Unified Workflow Orchestration Evaluation System
ç»Ÿä¸€çš„å·¥ä½œæµç¼–æ’è¯„æµ‹ç³»ç»Ÿ - æ”¯æŒåˆ†æ”¯ç»“æ„çš„å¤šç»´åº¦è¯„æµ‹

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è‡ªåŠ¨æ£€æµ‹å·¥ä½œæµç±»å‹ï¼ˆçº¿æ€§/åˆ†æ”¯ï¼‰
2. IDæ··æ·†å’Œæ–‡æœ¬æ¸…ç†
3. å¤šç»´åº¦è¯„æµ‹ï¼ˆé¡ºåºã€ä¾èµ–ã€æ•°æ®æµã€æ§åˆ¶æµã€æ‹“æ‰‘ï¼‰
4. è¯¦ç»†çš„è¯„æµ‹æŠ¥å‘Š
"""

import json
import random
import copy
from typing import Dict, List, Any
from pathlib import Path
from datetime import datetime
import sys
import re
import bisect
import asyncio
from tqdm import tqdm

# å‡è®¾è¿™äº›æ¨¡å—å­˜åœ¨
project_root = Path(__file__).resolve().parent.parent
sys.path.append(str(project_root))
from utils.llm_interface import LLMInterface
from core.config import LLMConfig
from utils.evaluate_tools import obfuscate_steps, parse_llm_output

# æ··æ·†å·¥å…·å·²ç§»è‡³ `utils.evaluate_tools.obfuscate_steps`

# ==================== ä¸»è¯„æµ‹ç³»ç»Ÿ ====================

class WorkflowEvaluationSystem:
    """ç»Ÿä¸€çš„å·¥ä½œæµè¯„æµ‹ç³»ç»Ÿ"""
    
    def __init__(self, reference_workflow_path: str, seed: int = 42, obfuscate: bool = True):
        self.reference_workflow = self._load_workflow(reference_workflow_path)
        self.seed = seed
        self.obfuscate = obfuscate
        random.seed(seed)
        
        # æå–å‚è€ƒç­”æ¡ˆ
        self.reference_answer = self._extract_reference_answer()
        self.id_mapping = None  # å­˜å‚¨æ··æ·†åçš„IDæ˜ å°„
        self.generated_workflow_steps = None  # å­˜å‚¨ç”Ÿæˆçš„å·¥ä½œæµæ­¥éª¤
    
    def _load_workflow(self, path: str) -> Dict:
        """åŠ è½½å‚è€ƒå·¥ä½œæµ"""
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _extract_reference_answer(self) -> Dict:
        """
        ä»å‚è€ƒå·¥ä½œæµä¸­æå–è¯„æµ‹æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯
        """
        steps = self.reference_workflow['workflow_steps']
        
        return {
            'execution_order': [s['step_id'] for s in steps],  # ä¿ç•™åŸå§‹é¡ºåºç”¨äºæ˜¾ç¤º
            'steps_detail': {s['step_id']: s for s in steps},
            'dependencies': self._extract_dependencies(steps),
            'dataflow': self._extract_dataflow(steps),
            'control_flow': self._extract_control_flow(steps),
            'step_count': len(steps),
            'has_branches': self._check_has_branches(steps)  # æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æœ‰åˆ†æ”¯
        }
    
    def _check_has_branches(self, steps: List[Dict]) -> bool:
        """æ£€æŸ¥å·¥ä½œæµæ˜¯å¦æœ‰åˆ†æ”¯ç»“æ„"""
        for step in steps:
            next_steps = step.get('next_steps', [])
            if len(next_steps) > 1:
                return True
        return False
    
    def _extract_dependencies(self, steps: List[Dict]) -> Dict[str, List[str]]:
        """æå–æ­¥éª¤é—´çš„ä¾èµ–å…³ç³»ï¼ˆnext_stepsï¼‰"""
        deps = {}
        for step in steps:
            if step.get('next_steps'):
                deps[step['step_id']] = step['next_steps']
        return deps
    
    def _extract_dataflow(self, steps: List[Dict]) -> Dict[str, List[str]]:
        """æå–æ•°æ®æµä¾èµ–ï¼ˆ{{step_X.output}}å¼•ç”¨ï¼‰"""
        dataflow = {}
        for step in steps:
            refs = self._find_step_references(step.get('parameters', {}))
            refs.extend(self._find_step_references(step.get('condition', '')))
            if refs:
                dataflow[step['step_id']] = list(set(refs))
        return dataflow
    
    def _extract_control_flow(self, steps: List[Dict]) -> Dict[str, Dict]:
        """æå–æ§åˆ¶æµä¿¡æ¯ï¼ˆelse_steps, error_handlerï¼‰"""
        control_flow = {}
        for step in steps:
            cf = {}
            if step.get('else_steps'):
                cf['else_steps'] = step['else_steps']
            if step.get('error_handler'):
                cf['error_handler'] = step['error_handler']
            if cf:
                control_flow[step['step_id']] = cf
        return control_flow
    
    def _find_step_references(self, obj: Any) -> List[str]:
        """é€’å½’æŸ¥æ‰¾æ‰€æœ‰{{step_<id>.*}}å¼•ç”¨"""
        refs = []
        if isinstance(obj, dict):
            for v in obj.values():
                refs.extend(self._find_step_references(v))
        elif isinstance(obj, list):
            for x in obj:
                refs.extend(self._find_step_references(x))
        elif isinstance(obj, str):
            # åŒ¹é… {{step_<id>...}} æˆ– {{ step_<id>... }}
            refs.extend(re.findall(r'\{\{\s*step_(\w+)[^}]*\}\}', obj))
        return refs
    
    def generate_shuffled_workflow(self) -> Dict:
        """
        ç”Ÿæˆæ‰“ä¹±é¡ºåºçš„æµ‹è¯•å·¥ä½œæµ
        
        æ­¥éª¤ï¼š
        1. æ·±æ‹·è´åŸå§‹å·¥ä½œæµ
        2. ï¼ˆå¯é€‰ï¼‰æ··æ·†æ­¥éª¤IDå’Œæ–‡æœ¬
        3. æ‰“ä¹±æ­¥éª¤é¡ºåº
        4. åˆ é™¤ next_steps å­—æ®µï¼ˆè®©LLMè‡ªå·±æ¨æ–­ï¼‰
        """
        shuffled = copy.deepcopy(self.reference_workflow)
        steps = shuffled['workflow_steps']
        
        # è®°å½•åŸå§‹IDé¡ºåº
        original_ids = [s['step_id'] for s in steps]
        
        # 1. æ··æ·†æ­¥éª¤IDå’Œæ–‡æœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.obfuscate:
            self.id_mapping = obfuscate_steps(steps)
            # æ›´æ–°å‚è€ƒç­”æ¡ˆä¸­çš„ID
            self.reference_answer['execution_order'] = [
                self.id_mapping[old_id] for old_id in original_ids
            ]
            # æ›´æ–°ä¾èµ–å…³ç³»ä¸­çš„ID
            self.reference_answer['dependencies'] = {
                self.id_mapping[k]: [self.id_mapping[v] for v in vs]
                for k, vs in self.reference_answer['dependencies'].items()
            }
            # æ›´æ–°æ•°æ®æµä¸­çš„ID
            self.reference_answer['dataflow'] = {
                self.id_mapping[k]: [self.id_mapping[v] for v in vs]
                for k, vs in self.reference_answer['dataflow'].items()
            }
            # æ›´æ–°æ§åˆ¶æµä¸­çš„ID
            new_control_flow = {}
            for k, v in self.reference_answer['control_flow'].items():
                new_k = self.id_mapping[k]
                new_v = {}
                if 'else_steps' in v:
                    new_v['else_steps'] = [self.id_mapping[x] for x in v['else_steps']]
                if 'error_handler' in v:
                    new_v['error_handler'] = self.id_mapping[v['error_handler']]
                new_control_flow[new_k] = new_v
            self.reference_answer['control_flow'] = new_control_flow
        
        # 2. æ‰“ä¹±æ­¥éª¤é¡ºåº
        random.shuffle(steps)
        
        # 3. åˆ é™¤ next_steps å­—æ®µï¼ˆä¿ç•™åœ¨å‚è€ƒç­”æ¡ˆä¸­ç”¨äºè¯„æµ‹ï¼‰
        for step in steps:
            step.pop('next_steps', None)
        
        shuffled['workflow_steps'] = steps
        return shuffled
        
    def evaluate(self, generated_workflow: Dict) -> Dict:
        """
        å¤šç»´åº¦è¯„æµ‹ç”Ÿæˆçš„å·¥ä½œæµ
        
        è¯„æµ‹ç»´åº¦ï¼š
        1. æ‰§è¡Œé¡ºåºå‡†ç¡®æ€§ (Execution Order Accuracy)
        2. ä¾èµ–å…³ç³»å‡†ç¡®æ€§ (Dependency Accuracy) 
        3. æ•°æ®æµæœ‰æ•ˆæ€§ (Dataflow Validity)
        4. æ§åˆ¶æµå‡†ç¡®æ€§ (Control Flow Accuracy)
        5. æ‹“æ‰‘æœ‰æ•ˆæ€§ (Topology Validity)
        """
        gen_steps = generated_workflow['workflow_steps']
        gen_order = [s['step_id'] for s in gen_steps]
        
        # ä¿å­˜ç”Ÿæˆçš„æ­¥éª¤ä¿¡æ¯ï¼Œä¾›åç»­ä½¿ç”¨
        self.generated_workflow_steps = gen_steps
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'reference_order': self.reference_answer['execution_order'],
            'generated_order': gen_order,
            'metrics': {}
        }
        
        # 1. æ‰§è¡Œé¡ºåºå‡†ç¡®æ€§
        results['metrics']['order_accuracy'] = self._evaluate_order(gen_order)
        
        # 2. ä¾èµ–å…³ç³»å‡†ç¡®æ€§
        results['metrics']['dependency_accuracy'] = self._evaluate_dependencies(gen_steps)
        
        # 3. æ•°æ®æµæœ‰æ•ˆæ€§
        # results['metrics']['dataflow_validity'] = self._evaluate_dataflow(gen_steps)
        
        # 4. æ§åˆ¶æµå‡†ç¡®æ€§
        results['metrics']['control_flow_accuracy'] = self._evaluate_control_flow(gen_steps)
        
        # 5. æ‹“æ‰‘æœ‰æ•ˆæ€§
        # results['metrics']['topology_validity'] = self._evaluate_topology(gen_steps)
        
        # è®¡ç®—æ€»åˆ†
        results['overall_score'] = self._calculate_overall_score(results['metrics'])
        
        return results
    
    def _evaluate_order(self, generated_order: List[str]) -> Dict:
        """
        è¯„æµ‹æ‰§è¡Œé¡ºåºå‡†ç¡®æ€§
        - çº¿æ€§å·¥ä½œæµï¼šä¸¥æ ¼åºåˆ—åŒ¹é…
        - åˆ†æ”¯å·¥ä½œæµï¼šåˆ†æ”¯ç»“æ„åŒ¹é… + çº¿æ€§åºåˆ—éªŒè¯
        """
        ref_order = self.reference_answer['execution_order']
        ref_deps = self.reference_answer['dependencies']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æ”¯ç»“æ„
        has_branches = any(len(nexts) > 1 for nexts in ref_deps.values())
        
        if has_branches:
            # å¯¹äºåˆ†æ”¯ç»“æ„ï¼Œä½¿ç”¨åˆ†æ”¯å·¥ä½œæµè¯„æµ‹
            return self._evaluate_branching_workflow(self.generated_workflow_steps)
        else:
            # å¯¹äºçº¿æ€§ç»“æ„ï¼Œä½¿ç”¨ä¸¥æ ¼çš„åºåˆ—åŒ¹é…
            return self._evaluate_linear_order(generated_order, ref_order)
    
    def _evaluate_branching_workflow(self, gen_steps: List[Dict]) -> Dict:
        """
        é’ˆå¯¹åˆ†æ”¯å·¥ä½œæµçš„è¯„æµ‹
        è¿”å›4ä¸ªç‹¬ç«‹æŒ‡æ ‡ï¼š
        1. æ•´ä½“è·¯å¾„æ­£ç¡®æ€§ (overall_path_correctness)
        2. ä¸»è·¯å¾„å‡†ç¡®åº¦ (main_path_accuracy)
        3. åˆ†æ”¯ç»“æ„æ­£ç¡®æ€§ (branch_structure_correctness)
        4. åˆ†æ”¯è·¯å¾„å‡†ç¡®åº¦ (branch_path_accuracy)
        """
        ref_deps = self.reference_answer['dependencies']
        ref_order = self.reference_answer['execution_order']
        
        # æ„å»ºç”Ÿæˆå·¥ä½œæµçš„ä¾èµ–å›¾
        gen_deps = {}
        for step in gen_steps:
            if step.get('next_steps'):
                gen_deps[step['step_id']] = step['next_steps']
        
        gen_order = [s['step_id'] for s in gen_steps]
        
        results = {
            'evaluation_type': 'branching_workflow'
        }
        
        # 1. è¯†åˆ«å‚è€ƒç­”æ¡ˆçš„æ‰€æœ‰åˆ†æ”¯ç‚¹
        ref_branch_points = [(k, v) for k, v in ref_deps.items() if len(v) > 1]
        
        if not ref_branch_points:
            return self._evaluate_linear_order(gen_order, ref_order)
        
        # å‡è®¾åªæœ‰ä¸€ä¸ªä¸»åˆ†æ”¯ç‚¹
        first_branch_point_id, ref_branch_targets = ref_branch_points[0]
        
        # ========== æŒ‡æ ‡1: ä¸»è·¯å¾„å‡†ç¡®åº¦ ==========
        main_path_result = self._evaluate_main_path(
            first_branch_point_id, 
            ref_order, 
            gen_order
        )
        results['main_path_accuracy'] = main_path_result['accuracy']
        results['main_path_details'] = main_path_result
        
        # ========== æŒ‡æ ‡2: åˆ†æ”¯ç»“æ„æ­£ç¡®æ€§ ==========
        branch_structure_result = self._evaluate_branch_structure(
            first_branch_point_id,
            ref_branch_targets,
            gen_deps
        )
        results['branch_structure_correctness'] = 1.0 if branch_structure_result['correct'] else 0.0
        results['branch_structure_details'] = branch_structure_result
        
        # ========== æŒ‡æ ‡3: åˆ†æ”¯è·¯å¾„å‡†ç¡®åº¦ ==========
        branch_paths_result = self._evaluate_branch_paths(
            ref_branch_targets,
            ref_deps,
            gen_deps,
            gen_order
        )
        results['branch_path_accuracy'] = branch_paths_result['average_accuracy']
        results['branch_path_details'] = branch_paths_result
        
        # ========== æŒ‡æ ‡4: æ•´ä½“è·¯å¾„æ­£ç¡®æ€§ ==========
        # ç»¼åˆè€ƒè™‘ï¼šä¾èµ–æ»¡è¶³åº¦ + å®Œæ•´æ€§
        dependency_result = self._evaluate_dependency_satisfaction(
            ref_deps,
            gen_order
        )
        
        # æ£€æŸ¥æ­¥éª¤å®Œæ•´æ€§
        ref_steps_set = set(ref_order)
        gen_steps_set = set(gen_order)
        missing_steps = ref_steps_set - gen_steps_set
        extra_steps = gen_steps_set - ref_steps_set
        
        completeness = len(ref_steps_set & gen_steps_set) / len(ref_steps_set) if ref_steps_set else 1.0
        
        # æ•´ä½“è·¯å¾„æ­£ç¡®æ€§ = ä¾èµ–æ»¡è¶³åº¦ * å®Œæ•´æ€§
        overall_path_correctness = dependency_result['satisfaction_rate'] * completeness
        
        results['overall_path_correctness'] = overall_path_correctness
        results['overall_path_details'] = {
            'dependency_satisfaction': dependency_result['satisfaction_rate'],
            'completeness': completeness,
            'missing_steps': list(missing_steps),
            'extra_steps': list(extra_steps),
            'satisfied_dependencies': dependency_result['satisfied_dependencies'],
            'total_dependencies': dependency_result['total_dependencies'],
            'violations': dependency_result['violations']
        }
        
        return results
    
    def _evaluate_main_path(self, branch_point_id: str, ref_order: List[str], gen_order: List[str]) -> Dict:
        """
        è¯„æµ‹ä¸»è·¯å¾„ï¼ˆä»èµ·ç‚¹åˆ°åˆ†æ”¯ç‚¹çš„çº¿æ€§åºåˆ—ï¼‰
        """
        # æå–å‚è€ƒç­”æ¡ˆçš„ä¸»è·¯å¾„
        try:
            branch_index = ref_order.index(branch_point_id)
            ref_main_path = ref_order[:branch_index + 1]
        except ValueError:
            return {
                'accuracy': 0.0,
                'error': f'Branch point {branch_point_id} not found in reference order'
            }
        
        # æå–ç”Ÿæˆç­”æ¡ˆçš„ä¸»è·¯å¾„ï¼ˆæŒ‰å‡ºç°é¡ºåºæ”¶é›†ç›¸åŒçš„æ­¥éª¤ï¼‰
        gen_main_path = []
        for step_id in gen_order:
            if step_id in ref_main_path:
                gen_main_path.append(step_id)
            if step_id == branch_point_id:
                break
        
        # æ£€æŸ¥æ˜¯å¦å®Œå…¨åŒ¹é…
        exact_match = (gen_main_path == ref_main_path)
        
        # è®¡ç®—LCSç›¸ä¼¼åº¦
        lcs_similarity = self._calculate_lcs_similarity(gen_main_path, ref_main_path)
        
        # è®¡ç®—ä½ç½®åå·®
        position_errors = []
        ref_positions = {sid: i for i, sid in enumerate(ref_main_path)}
        
        for i, step_id in enumerate(gen_main_path):
            if step_id in ref_positions:
                error = abs(i - ref_positions[step_id])
                position_errors.append(error)
        
        return {
            'reference_path': ref_main_path,
            'generated_path': gen_main_path,
            'exact_match': exact_match,
            'lcs_similarity': lcs_similarity,
            'accuracy': lcs_similarity,
            'position_errors': {
                'mean': sum(position_errors) / len(position_errors) if position_errors else 0.0,
                'max': max(position_errors) if position_errors else 0,
                'total': sum(position_errors)
            }
        }
    
    def _evaluate_branch_structure(self, branch_point_id: str, ref_branch_targets: List[str], gen_deps: Dict[str, List[str]]) -> Dict:
        """
        è¯„æµ‹åˆ†æ”¯ç»“æ„æ˜¯å¦æ­£ç¡®
        æ£€æŸ¥ç”Ÿæˆçš„å·¥ä½œæµåœ¨åˆ†æ”¯ç‚¹æ˜¯å¦æœ‰æ­£ç¡®çš„åˆ†æ”¯ç›®æ ‡
        """
        gen_branch_targets = gen_deps.get(branch_point_id, [])
        
        ref_targets_set = set(ref_branch_targets)
        gen_targets_set = set(gen_branch_targets)
        
        # å®Œå…¨åŒ¹é…
        exact_match = (ref_targets_set == gen_targets_set)
        
        # éƒ¨åˆ†åŒ¹é…
        intersection = ref_targets_set & gen_targets_set
        union = ref_targets_set | gen_targets_set
        
        jaccard_similarity = len(intersection) / len(union) if union else 0.0
        
        missing_branches = list(ref_targets_set - gen_targets_set)
        extra_branches = list(gen_targets_set - ref_targets_set)
        
        return {
            'correct': exact_match,
            'reference_branches': ref_branch_targets,
            'generated_branches': gen_branch_targets,
            'jaccard_similarity': jaccard_similarity,
            'missing_branches': missing_branches,
            'extra_branches': extra_branches,
            'matched_branches': list(intersection)
        }
    
    def _evaluate_branch_paths(self, ref_branch_targets: List[str], ref_deps: Dict[str, List[str]], gen_deps: Dict[str, List[str]], gen_order: List[str]) -> Dict:
        """
        è¯„æµ‹æ¯ä¸ªåˆ†æ”¯è·¯å¾„çš„å‡†ç¡®æ€§
        """
        branch_results = {}
        accuracies = []
        
        for branch_start_id in ref_branch_targets:
            # æå–å‚è€ƒåˆ†æ”¯çš„å®Œæ•´è·¯å¾„
            ref_branch_path = self._extract_linear_path(branch_start_id, ref_deps)
            
            # æå–ç”Ÿæˆåˆ†æ”¯çš„å®Œæ•´è·¯å¾„
            gen_branch_path = self._extract_linear_path(branch_start_id, gen_deps)
            
            # å¦‚æœç”Ÿæˆçš„å·¥ä½œæµä¸­æ²¡æœ‰è¿™ä¸ªåˆ†æ”¯èµ·ç‚¹ï¼Œæ ‡è®°ä¸ºç¼ºå¤±
            if branch_start_id not in gen_deps and branch_start_id not in [s for s in gen_order]:
                branch_results[branch_start_id] = {
                    'reference_path': ref_branch_path,
                    'generated_path': [],
                    'accuracy': 0.0,
                    'status': 'missing'
                }
                accuracies.append(0.0)
                continue
            
            # æ£€æŸ¥å®Œå…¨åŒ¹é…
            exact_match = (gen_branch_path == ref_branch_path)
            
            # è®¡ç®—LCSç›¸ä¼¼åº¦
            lcs_similarity = self._calculate_lcs_similarity(gen_branch_path, ref_branch_path)
            
            # è®¡ç®—ä½ç½®åå·®ï¼ˆåœ¨æ•´ä¸ªå·¥ä½œæµä¸­çš„ç›¸å¯¹ä½ç½®ï¼‰
            ref_positions = {sid: i for i, sid in enumerate(ref_branch_path)}
            gen_positions = {sid: i for i, sid in enumerate(gen_branch_path)}
            
            position_errors = []
            for step_id in ref_branch_path:
                if step_id in gen_positions:
                    error = abs(gen_positions[step_id] - ref_positions[step_id])
                    position_errors.append(error)
            
            branch_results[branch_start_id] = {
                'reference_path': ref_branch_path,
                'generated_path': gen_branch_path,
                'exact_match': exact_match,
                'lcs_similarity': lcs_similarity,
                'accuracy': lcs_similarity,
                'position_errors': {
                    'mean': sum(position_errors) / len(position_errors) if position_errors else 0.0,
                    'max': max(position_errors) if position_errors else 0,
                },
                'status': 'evaluated'
            }
            
            accuracies.append(lcs_similarity)
        
        average_accuracy = sum(accuracies) / len(accuracies) if accuracies else 0.0
        
        return {
            'branches': branch_results,
            'average_accuracy': average_accuracy,
            'total_branches': len(ref_branch_targets),
            'evaluated_branches': len(accuracies)
        }
    
    def _extract_linear_path(self, start_step: str, dependencies: Dict[str, List[str]]) -> List[str]:
        """
        ä»æŸä¸ªæ­¥éª¤å¼€å§‹ï¼Œæå–åˆ°ç»ˆç‚¹çš„å®Œæ•´çº¿æ€§è·¯å¾„
        å¦‚æœé‡åˆ°åˆ†æ”¯ç‚¹ï¼Œåªå–ç¬¬ä¸€ä¸ªåç»§ï¼ˆå‡è®¾æ¯ä¸ªåˆ†æ”¯å†…éƒ¨æ˜¯çº¿æ€§çš„ï¼‰
        """
        path = [start_step]
        current = start_step
        visited = {start_step}  # é˜²æ­¢å¾ªç¯
        
        while current in dependencies and dependencies[current]:
            next_steps = dependencies[current]
            
            if not next_steps:
                break
            
            # å–ç¬¬ä¸€ä¸ªåç»§æ­¥éª¤
            next_step = next_steps[0]
            
            # é˜²æ­¢å¾ªç¯
            if next_step in visited:
                break
            
            path.append(next_step)
            visited.add(next_step)
            current = next_step
        
        return path
    
    def _evaluate_dependency_satisfaction(self, ref_deps: Dict[str, List[str]], gen_order: List[str]) -> Dict:
        """
        è¯„æµ‹ä¾èµ–å…³ç³»çš„æ»¡è¶³åº¦
        æ£€æŸ¥ç”Ÿæˆçš„é¡ºåºæ˜¯å¦æ»¡è¶³å‚è€ƒç­”æ¡ˆä¸­çš„æ‰€æœ‰ä¾èµ–å…³ç³»
        """
        gen_positions = {step_id: i for i, step_id in enumerate(gen_order)}
        
        violations = []
        satisfied_deps = 0
        total_deps = 0
        
        for step_id, next_steps in ref_deps.items():
            for next_id in next_steps:
                total_deps += 1
                
                if step_id not in gen_positions or next_id not in gen_positions:
                    # å¦‚æœæ­¥éª¤ç¼ºå¤±ï¼Œè®°å½•ä¸ºè¿è§„
                    violations.append({
                        'from': step_id,
                        'to': next_id,
                        'reason': f'Missing step: {step_id if step_id not in gen_positions else next_id}'
                    })
                    continue
                
                # æ£€æŸ¥ä¾èµ–å…³ç³»ï¼šstep_id å¿…é¡»åœ¨ next_id ä¹‹å‰
                if gen_positions[step_id] < gen_positions[next_id]:
                    satisfied_deps += 1
                else:
                    violations.append({
                        'from': step_id,
                        'to': next_id,
                        'from_pos': gen_positions[step_id],
                        'to_pos': gen_positions[next_id],
                        'reason': f'Dependency violated: {step_id} must come before {next_id}'
                    })
        
        satisfaction_rate = satisfied_deps / total_deps if total_deps > 0 else 1.0
        
        return {
            'satisfaction_rate': satisfaction_rate,
            'satisfied_dependencies': satisfied_deps,
            'total_dependencies': total_deps,
            'violations': violations,
            'is_valid': len(violations) == 0
        }
    
    def _evaluate_linear_order(self, generated_order: List[str], ref_order: List[str]) -> Dict:
        """
        è¯„æµ‹çº¿æ€§é¡ºåº
        å¯¹äºçº¿æ€§å·¥ä½œæµï¼Œåªè¿”å›ä¸€ä¸ªæ•´ä½“å‡†ç¡®åº¦
        """
        # å®Œå…¨åŒ¹é…æ£€æŸ¥
        exact_match = (generated_order == ref_order)
        
        # LCSç›¸ä¼¼åº¦
        lcs_score = self._calculate_lcs_similarity(generated_order, ref_order)
        
        # ä½ç½®åå·®ç»Ÿè®¡
        position_errors = self._calculate_position_errors(generated_order, ref_order)
        
        # æ£€æŸ¥æ­¥éª¤å®Œæ•´æ€§
        ref_steps_set = set(ref_order)
        gen_steps_set = set(generated_order)
        completeness = len(ref_steps_set & gen_steps_set) / len(ref_steps_set) if ref_steps_set else 1.0
        
        return {
            'evaluation_type': 'linear',
            'exact_match': exact_match,
            'overall_path_correctness': lcs_score * completeness,
            'lcs_similarity': lcs_score,
            'completeness': completeness,
            'position_errors': position_errors
        }
    
    def _calculate_lcs_similarity(self, seq1: List[str], seq2: List[str]) -> float:
        """è®¡ç®—ä¸¤ä¸ªåºåˆ—çš„LCSç›¸ä¼¼åº¦"""
        if not seq1 or not seq2:
            return 0.0
        
        # ç¡®ä¿ seq2 æ˜¯è¾ƒçŸ­çš„åºåˆ—ä»¥èŠ‚çœç©ºé—´
        if len(seq1) < len(seq2):
            seq1, seq2 = seq2, seq1
        
        m, n = len(seq1), len(seq2)
        prev = [0] * (n + 1)
        curr = [0] * (n + 1)
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if seq1[i-1] == seq2[j-1]:
                    curr[j] = prev[j-1] + 1
                else:
                    curr[j] = max(prev[j], curr[j-1])
            prev, curr = curr, prev
        
        lcs_length = prev[n]
        return lcs_length / max(m, n)

    def _calculate_lis_similarity(self, seq1: List[str], seq2: List[str]) -> float:
        """è®¡ç®—ä¸¤ä¸ªåºåˆ—çš„LISç›¸ä¼¼åº¦"""
        if not seq1 or not seq2:
            return 0.0
        
        pos_map = {val: i for i, val in enumerate(seq2)}
        mapped_seq = [pos_map[val] for val in seq1 if val in pos_map]
        
        lis = []
        for val in mapped_seq:
            idx = bisect.bisect_left(lis, val)
            if idx == len(lis):
                lis.append(val)
            else:
                lis[idx] = val
        
        lis_length = len(lis)
        return lis_length / max(len(seq1), len(seq2))
    
    def _calculate_position_errors(self, gen_order: List[str], ref_order: List[str]) -> Dict:
        """è®¡ç®—ä½ç½®åå·®"""
        ref_positions = {sid: i for i, sid in enumerate(ref_order)}
        
        errors = []
        for i, sid in enumerate(gen_order):
            if sid in ref_positions:
                error = abs(i - ref_positions[sid])
                errors.append(error)
        
        if not errors:
            return {'mean': 0, 'max': 0, 'total': 0}
        
        return {
            'mean': sum(errors) / len(errors),
            'max': max(errors),
            'total': sum(errors)
        }
    
    def _evaluate_dependencies(self, gen_steps: List[Dict]) -> Dict:
        """è¯„æµ‹ä¾èµ–å…³ç³»å‡†ç¡®æ€§ï¼ˆnext_stepsï¼‰"""
        ref_deps = self.reference_answer['dependencies']
        
        # æ„å»ºç”Ÿæˆå·¥ä½œæµçš„ä¾èµ–å›¾
        gen_deps = {}
        for step in gen_steps:
            if step.get('next_steps'):
                gen_deps[step['step_id']] = set(step['next_steps'])
        
        # ç»Ÿè®¡å‡†ç¡®æ€§
        total_edges = 0
        correct_edges = 0
        missing_edges = []
        extra_edges = []
        
        for sid, ref_nexts in ref_deps.items():
            for next_id in ref_nexts:
                total_edges += 1
                if next_id in gen_deps.get(sid, set()):
                    correct_edges += 1
                else:
                    missing_edges.append((sid, next_id))
        
        # æ£€æŸ¥å¤šä½™çš„è¾¹
        for sid, gen_nexts in gen_deps.items():
            for next_id in gen_nexts:
                if sid not in ref_deps or next_id not in ref_deps[sid]:
                    extra_edges.append((sid, next_id))
        
        accuracy = correct_edges / total_edges if total_edges > 0 else 1.0
        
        return {
            'accuracy': accuracy,
            'total_edges': total_edges,
            'correct_edges': correct_edges,
            'missing_edges': missing_edges,
            'extra_edges': extra_edges,
            'score': accuracy
        }
    
    # def _evaluate_dataflow(self, gen_steps: List[Dict]) -> Dict:
    #     """è¯„æµ‹æ•°æ®æµæœ‰æ•ˆæ€§"""
    #     # æ„å»ºä½ç½®æ˜ å°„
    #     positions = {s['step_id']: i for i, s in enumerate(gen_steps)}
        
    #     violations = []
    #     valid_refs = 0
    #     total_refs = 0
        
    #     # æ£€æŸ¥æ¯ä¸ªæ­¥éª¤çš„æ•°æ®ä¾èµ–
    #     for step in gen_steps:
    #         step_id = step['step_id']
    #         refs = self._find_step_references(step.get('parameters', {}))
    #         refs.extend(self._find_step_references(step.get('condition', '')))
            
    #         for ref_id in refs:
    #             total_refs += 1
    #             if ref_id in positions:
    #                 # ç”Ÿäº§è€…å¿…é¡»åœ¨æ¶ˆè´¹è€…ä¹‹å‰
    #                 if positions[ref_id] < positions[step_id]:
    #                     valid_refs += 1
    #                 else:
    #                     violations.append({
    #                         'consumer': step_id,
    #                         'producer': ref_id,
    #                         'consumer_pos': positions[step_id],
    #                         'producer_pos': positions[ref_id]
    #                     })
        
    #     is_valid = len(violations) == 0
    #     validity_score = valid_refs / total_refs if total_refs > 0 else 1.0
        
    #     return {
    #         'is_valid': is_valid,
    #         'validity_score': validity_score,
    #         'total_references': total_refs,
    #         'valid_references': valid_refs,
    #         'violations': violations,
    #         'score': validity_score
    #     }
    
    def _evaluate_control_flow(self, gen_steps: List[Dict]) -> Dict:
        """è¯„æµ‹æ§åˆ¶æµå‡†ç¡®æ€§ï¼ˆelse_steps, error_handlerï¼‰"""
        ref_control = self.reference_answer['control_flow']
        
        if not ref_control:
            return {'accuracy': 1.0, 'score': 1.0, 'details': 'No control flow in reference'}
        
        # æ„å»ºç”Ÿæˆå·¥ä½œæµçš„æ§åˆ¶æµ
        gen_control = {}
        for step in gen_steps:
            cf = {}
            if step.get('else_steps'):
                cf['else_steps'] = set(step['else_steps'])
            if step.get('error_handler'):
                cf['error_handler'] = step['error_handler']
            if cf:
                gen_control[step['step_id']] = cf
        
        # ç»Ÿè®¡å‡†ç¡®æ€§
        total_controls = 0
        correct_controls = 0
        details = {'else_steps': {}, 'error_handler': {}}
        
        for sid, ref_cf in ref_control.items():
            gen_cf = gen_control.get(sid, {})
            
            # æ£€æŸ¥else_steps
            if 'else_steps' in ref_cf:
                total_controls += len(ref_cf['else_steps'])
                ref_else = set(ref_cf['else_steps'])
                gen_else = gen_cf.get('else_steps', set())
                correct = len(ref_else & gen_else)
                correct_controls += correct
                details['else_steps'][sid] = {
                    'expected': list(ref_else),
                    'actual': list(gen_else),
                    'correct': correct
                }
            
            # æ£€æŸ¥error_handler
            if 'error_handler' in ref_cf:
                total_controls += 1
                if gen_cf.get('error_handler') == ref_cf['error_handler']:
                    correct_controls += 1
                    details['error_handler'][sid] = 'correct'
                else:
                    details['error_handler'][sid] = {
                        'expected': ref_cf['error_handler'],
                        'actual': gen_cf.get('error_handler', None)
                    }

        accuracy = correct_controls / total_controls if total_controls > 0 else 1.0
        return {
            'accuracy': accuracy,
            'total_controls': total_controls,
            'correct_controls': correct_controls,
            'details': details,
            'score': accuracy
        }     

    # def _evaluate_topology(self, gen_steps: List[Dict]) -> Dict:
    #     """è¯„æµ‹æ‹“æ‰‘æœ‰æ•ˆæ€§"""
    #     positions = {s['step_id']: i for i, s in enumerate(gen_steps)}
        
    #     violations = {
    #         'backward_next_steps': [],
    #         'cycles': []
    #     }
        
    #     # 1. æ£€æŸ¥next_stepså¿…é¡»å‰å‘
    #     for step in gen_steps:
    #         step_id = step['step_id']
    #         for next_id in step.get('next_steps', []):
    #             if next_id in positions:
    #                 if positions[step_id] >= positions[next_id]:
    #                     violations['backward_next_steps'].append({
    #                         'from': step_id,
    #                         'to': next_id,
    #                         'from_pos': positions[step_id],
    #                         'to_pos': positions[next_id]
    #                     })
        
    #     # 2. æ£€æµ‹å¾ªç¯ï¼ˆé€šè¿‡DFSï¼‰
    #     cycles = self._detect_cycles(gen_steps)
    #     violations['cycles'] = cycles
        
    #     is_valid = (
    #         len(violations['backward_next_steps']) == 0 and
    #         len(violations['cycles']) == 0
    #     )
        
    #     # è®¡ç®—æ‹“æ‰‘åˆ†æ•°
    #     total_issues = len(violations['backward_next_steps']) + len(violations['cycles'])
    #     topology_score = 1.0 if is_valid else max(0.0, 1.0 - total_issues * 0.1)
        
    #     return {
    #         'is_valid': is_valid,
    #         'violations': violations,
    #         'score': topology_score
    #     }
    
    # def _detect_cycles(self, steps: List[Dict]) -> List[List[str]]:
    #     """ä½¿ç”¨Tarjanç®—æ³•æ£€æµ‹å¼ºè¿é€šåˆ†é‡ï¼ˆå¾ªç¯ï¼‰"""
    #     # æ„å»ºé‚»æ¥è¡¨
    #     graph = {}
    #     for s in steps:
    #         sid = s['step_id']
    #         graph[sid] = []
    #         graph[sid].extend(s.get('next_steps', []))
    #         graph[sid].extend(s.get('else_steps', []))
    #         if s.get('error_handler'):
    #             graph[sid].append(s['error_handler'])
        
    #     # Tarjanç®—æ³•
    #     index_counter = [0]
    #     stack = []
    #     lowlinks = {}
    #     index = {}
    #     on_stack = set()
    #     sccs = []
        
    #     def strongconnect(node):
    #         index[node] = index_counter[0]
    #         lowlinks[node] = index_counter[0]
    #         index_counter[0] += 1
    #         stack.append(node)
    #         on_stack.add(node)
            
    #         for successor in graph.get(node, []):
    #             if successor not in index:
    #                 strongconnect(successor)
    #                 lowlinks[node] = min(lowlinks[node], lowlinks[successor])
    #             elif successor in on_stack:
    #                 lowlinks[node] = min(lowlinks[node], index[successor])
            
    #         if lowlinks[node] == index[node]:
    #             scc = []
    #             while True:
    #                 successor = stack.pop()
    #                 on_stack.remove(successor)
    #                 scc.append(successor)
    #                 if successor == node:
    #                     break
    #             if len(scc) > 1:  # åªè®°å½•çœŸæ­£çš„å¾ªç¯
    #                 sccs.append(scc)
        
    #     for node in graph:
    #         if node not in index:
    #             strongconnect(node)
        
    #     return sccs
    
    def _calculate_overall_score(self, metrics: Dict) -> float:
        """
        è®¡ç®—æ€»åˆ†
        """
        order_metrics = metrics.get('order_accuracy', {})
        
        if order_metrics.get('evaluation_type') == 'branching_workflow':
            # å¯¹äºåˆ†æ”¯å·¥ä½œæµï¼Œä½¿ç”¨4ä¸ªç‹¬ç«‹æŒ‡æ ‡çš„åŠ æƒå¹³å‡
            weights = {
                'overall_path_correctness': 0.30,
                'main_path_accuracy': 0.25,
                'branch_structure_correctness': 0.20,
                'branch_path_accuracy': 0.25
            }
            
            order_score = sum(
                order_metrics.get(key, 0.0) * weight 
                for key, weight in weights.items()
            )
        else:
            # çº¿æ€§å·¥ä½œæµ
            order_score = order_metrics.get('overall_path_correctness', 0.0)
        
        # å…¶ä»–æŒ‡æ ‡
        weights = {
            'order': 0.50,  # æ‰§è¡Œé¡ºåºå 50%
            'dependency_accuracy': 0.25,
            # 'dataflow_validity': 0.15,
            'control_flow_accuracy': 0.25,
            # 'topology_validity': 0.075
        }
        
        total_score = (
            weights['order'] * order_score +
            weights['dependency_accuracy'] * metrics.get('dependency_accuracy', {}).get('score', 0.0) +
            # weights['dataflow_validity'] * metrics.get('dataflow_validity', {}).get('score', 0.0) +
            weights['control_flow_accuracy'] * metrics.get('control_flow_accuracy', {}).get('score', 0.0)
            # + weights['topology_validity'] * metrics.get('topology_validity', {}).get('score', 0.0)
        )
        
        return total_score
    
    def generate_report(self, results: Dict, output_path: Path = None) -> str:
        """ç”Ÿæˆè¯¦ç»†è¯„æµ‹æŠ¥å‘Š"""
        lines = [
            "=" * 80,
            "Workflow Orchestration Evaluation Report",
            "=" * 80,
            f"Timestamp: {results['timestamp']}",
            "",
        ]
        
        # æ‰§è¡Œé¡ºåºè¯„æµ‹ç»“æœ
        order_metrics = results['metrics']['order_accuracy']
        
        if order_metrics.get('evaluation_type') == 'branching_workflow':
            lines.extend([
                "=" * 80,
                "EXECUTION ORDER EVALUATION (BRANCHING WORKFLOW)",
                "=" * 80,
                "",
            ])
            
            # ========== 4ä¸ªç‹¬ç«‹æŒ‡æ ‡ ==========
            lines.extend([
                "ğŸ“Š KEY METRICS:",
                "",
                f"1. Overall Path Correctness:      {order_metrics['overall_path_correctness']:.2%}",
                f"2. Main Path Accuracy:            {order_metrics['main_path_accuracy']:.2%}",
                f"3. Branch Structure Correctness:  {order_metrics['branch_structure_correctness']:.2%}",
                f"4. Branch Path Accuracy:          {order_metrics['branch_path_accuracy']:.2%}",
                "",
            ])
            
            # ========== è¯¦ç»†åˆ†æ ==========
            lines.extend([
                "=" * 80,
                "DETAILED ANALYSIS",
                "=" * 80,
                "",
            ])
            
            # 1. æ•´ä½“è·¯å¾„æ­£ç¡®æ€§è¯¦æƒ…
            overall_details = order_metrics['overall_path_details']
            lines.extend([
                "1ï¸âƒ£  OVERALL PATH CORRECTNESS",
                f"   - Dependency Satisfaction: {overall_details['dependency_satisfaction']:.2%}",
                f"   - Completeness: {overall_details['completeness']:.2%}",
                f"   - Satisfied Dependencies: {overall_details['satisfied_dependencies']}/{overall_details['total_dependencies']}",
            ])
            
            if overall_details['missing_steps']:
                lines.append(f"   - Missing Steps: {overall_details['missing_steps']}")
            if overall_details['extra_steps']:
                lines.append(f"   - Extra Steps: {overall_details['extra_steps']}")
            
            if overall_details['violations']:
                lines.append(f"   - Violations: {len(overall_details['violations'])}")
                for v in overall_details['violations'][:3]:
                    lines.append(f"     â€¢ {v['reason']}")
            
            lines.append("")
            
            # 2. ä¸»è·¯å¾„å‡†ç¡®åº¦è¯¦æƒ…
            main_path = order_metrics['main_path_details']
            lines.extend([
                "2ï¸âƒ£  MAIN PATH ACCURACY",
                f"   - Exact Match: {'âœ“' if main_path.get('exact_match', False) else 'âœ—'}",
                f"   - LCS Similarity: {main_path.get('lcs_similarity', 0):.2%}",
                f"   - Reference Path:",
                f"     {' -> '.join(main_path.get('reference_path', []))}",
                f"   - Generated Path:",
                f"     {' -> '.join(main_path.get('generated_path', []))}",
            ])
            
            pos_errors = main_path.get('position_errors', {})
            if pos_errors.get('total', 0) > 0:
                lines.extend([
                    f"   - Position Errors:",
                    f"     â€¢ Mean: {pos_errors.get('mean', 0):.2f}",
                    f"     â€¢ Max: {pos_errors.get('max', 0)}",
                ])
            
            lines.append("")
            
            # 3. åˆ†æ”¯ç»“æ„æ­£ç¡®æ€§è¯¦æƒ…
            branch_struct = order_metrics['branch_structure_details']
            lines.extend([
                "3ï¸âƒ£  BRANCH STRUCTURE CORRECTNESS",
                f"   - Exact Match: {'âœ“' if branch_struct.get('correct', False) else 'âœ—'}",
                f"   - Jaccard Similarity: {branch_struct.get('jaccard_similarity', 0):.2%}",
                f"   - Reference Branches: {branch_struct.get('reference_branches', [])}",
                f"   - Generated Branches: {branch_struct.get('generated_branches', [])}",
            ])
            
            if branch_struct.get('matched_branches'):
                lines.append(f"   - Matched Branches: {branch_struct['matched_branches']}")
            if branch_struct.get('missing_branches'):
                lines.append(f"   - Missing Branches: {branch_struct['missing_branches']}")
            if branch_struct.get('extra_branches'):
                lines.append(f"   - Extra Branches: {branch_struct['extra_branches']}")
            
            lines.append("")
            
            # 4. åˆ†æ”¯è·¯å¾„å‡†ç¡®åº¦è¯¦æƒ…
            branch_paths = order_metrics['branch_path_details']
            lines.extend([
                "4ï¸âƒ£  BRANCH PATH ACCURACY",
                f"   - Average Accuracy: {branch_paths.get('average_accuracy', 0):.2%}",
                f"   - Evaluated Branches: {branch_paths.get('evaluated_branches', 0)}/{branch_paths.get('total_branches', 0)}",
                "",
            ])
            
            for branch_id, branch_info in branch_paths.get('branches', {}).items():
                status_icon = "âœ“" if branch_info.get('exact_match', False) else "âœ—"
                lines.extend([
                    f"   Branch: {branch_id} {status_icon}",
                    f"   - Accuracy: {branch_info.get('accuracy', 0):.2%}",
                    f"   - Reference: {' -> '.join(branch_info.get('reference_path', []))}",
                    f"   - Generated: {' -> '.join(branch_info.get('generated_path', []))}",
                ])
                
                if branch_info.get('status') == 'missing':
                    lines.append(f"   - Status: MISSING")
                
                lines.append("")
        
        else:
            # çº¿æ€§å·¥ä½œæµ
            lines.extend([
                "=" * 80,
                "EXECUTION ORDER EVALUATION (LINEAR WORKFLOW)",
                "=" * 80,
                "",
                "ğŸ“Š KEY METRIC:",
                "",
                f"Overall Path Correctness: {order_metrics['overall_path_correctness']:.2%}",
                "",
                "=" * 80,
                "DETAILED ANALYSIS",
                "=" * 80,
                "",
                f"- Exact Match: {'âœ“' if order_metrics['exact_match'] else 'âœ—'}",
                f"- LCS Similarity: {order_metrics.get('lcs_similarity', 0):.2%}",
                f"- Completeness: {order_metrics.get('completeness', 0):.2%}",
                f"- Mean Position Error: {order_metrics['position_errors']['mean']:.2f}",
                f"- Max Position Error: {order_metrics['position_errors']['max']}",
                "",
            ])
        
        # å…¶ä»–æŒ‡æ ‡ï¼ˆ2-5ï¼‰
        lines.extend([
            "=" * 80,
            "OTHER METRICS",
            "=" * 80,
            "",
        ])
        
        dep_metrics = results['metrics']['dependency_accuracy']
        lines.extend([
            "Dependency Accuracy (next_steps)",
            f"- Accuracy: {dep_metrics['accuracy']:.2%}",
            f"- Correct Edges: {dep_metrics['correct_edges']}/{dep_metrics['total_edges']}",
            f"- Missing Edges: {len(dep_metrics['missing_edges'])}",
            f"- Extra Edges: {len(dep_metrics['extra_edges'])}",
            "",
        ])
        
        # df_metrics = results['metrics']['dataflow_validity']
        # lines.extend([
        #     "Dataflow Validity",
        #     f"- Valid: {'âœ“' if df_metrics['is_valid'] else 'âœ—'}",
        #     f"- Validity Score: {df_metrics['validity_score']:.2%}",
        #     f"- Valid References: {df_metrics['valid_references']}/{df_metrics['total_references']}",
        #     "",
        # ])
        
        cf_metrics = results['metrics']['control_flow_accuracy']
        lines.extend([
            "Control Flow Accuracy",
            f"- Accuracy: {cf_metrics['accuracy']:.2%}",
            "",
        ])
        
        # topo_metrics = results['metrics']['topology_validity']
        # lines.extend([
        #     "Topology Validity",
        #     f"- Valid: {'âœ“' if topo_metrics['is_valid'] else 'âœ—'}",
        #     f"- Score: {topo_metrics['score']:.2%}",
        #     "",
        # ])
        
        # å¯è§†åŒ–å¯¹æ¯”
        lines.extend([
            "=" * 80,
            "WORKFLOW STRUCTURE COMPARISON",
            "=" * 80,
            "",
        ])
        
        # æ„å»ºç”Ÿæˆå·¥ä½œæµçš„ä¾èµ–å›¾
        gen_deps = {}
        if hasattr(self, 'generated_workflow_steps'):
            for step in self.generated_workflow_steps:
                if step.get('next_steps'):
                    gen_deps[step['step_id']] = step['next_steps']
        
        # æ˜¾ç¤ºå‚è€ƒç»“æ„
        if self.reference_answer.get('has_branches'):
            lines.append("ğŸ“‹ Reference Structure:")
            ref_deps = self.reference_answer['dependencies']
            
            branch_points = [(k, v) for k, v in ref_deps.items() if len(v) > 1]
            if branch_points:
                branch_point_id, branch_targets = branch_points[0]
                branch_index = self.reference_answer['execution_order'].index(branch_point_id)
                main_path = self.reference_answer['execution_order'][:branch_index + 1]
                
                lines.append(f"  Main Path: {' -> '.join(main_path)}")
                lines.append(f"  Branches from {branch_point_id}:")
                
                for i, branch_id in enumerate(branch_targets, 1):
                    branch_path = self._extract_linear_path(branch_id, ref_deps)
                    lines.append(f"    Branch {i}: {' -> '.join(branch_path)}")
        else:
            lines.extend([
                "ğŸ“‹ Reference Order:",
                f"  {' -> '.join(results['reference_order'])}",
            ])
        
        lines.append("")
        
        # æ˜¾ç¤ºç”Ÿæˆç»“æ„
        has_gen_branches = any(len(nexts) > 1 for nexts in gen_deps.values())
        
        if has_gen_branches:
            lines.append("ğŸ¤– Generated Structure:")
            
            gen_branch_points = [(k, v) for k, v in gen_deps.items() if len(v) > 1]
            if gen_branch_points:
                gen_branch_point_id, gen_branch_targets = gen_branch_points[0]
                
                gen_order = results['generated_order']
                branch_index = gen_order.index(gen_branch_point_id)
                gen_main_path = gen_order[:branch_index + 1]
                
                lines.append(f"  Main Path: {' -> '.join(gen_main_path)}")
                lines.append(f"  Branches from {gen_branch_point_id}:")
                
                for i, branch_id in enumerate(gen_branch_targets, 1):
                    branch_path = self._extract_linear_path(branch_id, gen_deps)
                    lines.append(f"    Branch {i}: {' -> '.join(branch_path)}")
        else:
            lines.extend([
                "ğŸ¤– Generated Order:",
                f"  {' -> '.join(results['generated_order'])}",
            ])
        
        lines.extend([
            "",
            "=" * 80,
            "OVERALL SCORE",
            "=" * 80,
            f"Total Score: {results['overall_score']:.2%}",
            "",
        ])
        
        # æœ€ç»ˆåˆ¤å®š
        if results['overall_score'] >= 0.95:
            lines.append(f"âœ… EXCELLENT")
        elif results['overall_score'] >= 0.80:
            lines.append(f"âœ… PASSED")
        elif results['overall_score'] >= 0.60:
            lines.append(f"âš ï¸  NEEDS IMPROVEMENT")
        else:
            lines.append(f"âŒ FAILED")
        
        lines.append("=" * 80)
        
        report = "\n".join(lines)
        
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report)
        
        return report



# parse_llm_output å·²ç§»åŠ¨åˆ° `utils.evaluate_tools.parse_llm_output`


import asyncio
import json
import os
from pathlib import Path


async def main():
    print("=" * 80)
    print("Unified Workflow Orchestration Evaluation System - Batch Mode")
    print("=" * 80)

    # é…ç½®LLM
    llm_config = LLMConfig(
        provider="openai",
        model_name="gpt-4o-mini",
        api_key="sk-2p51ZI79J5X4OL6S343c17F08f3c432395C711608b2eB0D5",
        base_url="https://az.gptplus5.com/v1",
        temperature=0.7,
        max_tokens=2048,
        timeout=60.0,
        retry_attempts=3,
        retry_delay=2.0
    )
    llm_interface = LLMInterface(llm_config)

    # æ ¹ç›®å½•è·¯å¾„
    reference_workflow_root_path = Path('E:\\MCPBenchMark\\MCPFLow\\mcp_projects')
    
    # è¾“å‡ºæ ¹ç›®å½•
    output_root_dir = Path('./evaluation_output')
    output_root_dir.mkdir(exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰ mcp_project ç›®å½•
    mcp_projects = []
    for item in reference_workflow_root_path.iterdir():
        if item.is_dir() and item.name.startswith('mcp_project'):
            # æå–æ•°å­—åºå·
            try:
                project_num = int(item.name.replace('mcp_project', ''))
                mcp_projects.append((project_num, item))
            except ValueError:
                print(f"âš ï¸ Skipping invalid project name: {item.name}")
                continue
    
    # æŒ‰åºå·æ’åº
    mcp_projects.sort(key=lambda x: x[0])
    
    print(f"\nğŸ“ Found {len(mcp_projects)} mcp_project directories")
    
    # æ±‡æ€»ç»“æœ
    summary_results = {
        'total': len(mcp_projects),
        'success': 0,
        'skipped': 0,
        'failed': 0,
        'details': []
    }
    
    # éå†æ¯ä¸ªé¡¹ç›®
    for project_num, project_path in tqdm(mcp_projects, total=len(mcp_projects), desc="Processing Projects"):
        print(f"\n{'='*60}")
        print(f"Processing mcp_project{project_num}")
        print(f"{'='*60}")
        
        # åˆ›å»ºè¯¥é¡¹ç›®çš„è¾“å‡ºç›®å½•ï¼ˆç”¨æ•°å­—åºå·å‘½åï¼‰
        project_output_dir = output_root_dir / str(project_num)
        project_output_dir.mkdir(exist_ok=True)
        
        workflow_path = project_path / 'workflow.json'
        
        # æ£€æŸ¥ workflow.json æ˜¯å¦å­˜åœ¨
        if not workflow_path.exists():
            skip_msg = f"workflow.json not found in mcp_project{project_num}"
            print(f"âš ï¸ {skip_msg}")
            
            # è®°å½•è·³è¿‡ä¿¡æ¯
            with open(project_output_dir / 'skipped.txt', 'w', encoding='utf-8') as f:
                f.write(f"Skipped: {skip_msg}\n")
                f.write(f"Project path: {project_path}\n")
            
            summary_results['skipped'] += 1
            summary_results['details'].append({
                'project_num': project_num,
                'status': 'skipped',
                'reason': 'workflow.json not found'
            })
            continue
        
        try:
            # åˆå§‹åŒ–è¯„æµ‹ç³»ç»Ÿï¼ˆå¯ç”¨æ··æ·†ï¼‰
            eval_system = WorkflowEvaluationSystem(
                reference_workflow_path=str(workflow_path),
                seed=42,
                obfuscate=True
            )
            
            # ç”Ÿæˆæ‰“ä¹±çš„æµ‹è¯•å·¥ä½œæµ
            shuffled_workflow = eval_system.generate_shuffled_workflow()
            
            # ä¿å­˜æµ‹è¯•ç”¨ä¾‹
            with open(project_output_dir / 'shuffled_workflow.json', 'w', encoding='utf-8') as f:
                json.dump(shuffled_workflow, f, indent=2, ensure_ascii=False)
            
            with open(project_output_dir / 'reference_answer.json', 'w', encoding='utf-8') as f:
                json.dump(eval_system.reference_answer, f, indent=2, ensure_ascii=False)
            
            print(f"\nâœ… Generated shuffled workflow (Obfuscated: {eval_system.obfuscate})")
            print(f"   Steps: {len(shuffled_workflow['workflow_steps'])}")
            
            # æ ¹æ®æ˜¯å¦æœ‰åˆ†æ”¯æ˜¾ç¤ºä¸åŒçš„ä¿¡æ¯
            if eval_system.reference_answer.get('has_branches'):
                print(f"   Workflow Type: BRANCHING (DAG)")
                print(f"   Main Path: {' -> '.join(eval_system.reference_answer['execution_order'][:5])}")
                if len(eval_system.reference_answer['execution_order']) > 4:
                    print(f"   Branch Point: Step 5 ({eval_system.reference_answer['execution_order'][4]})")
                    
                    deps = eval_system.reference_answer['dependencies']
                    branch_point = eval_system.reference_answer['execution_order'][4]
                    if branch_point in deps and len(deps[branch_point]) > 1:
                        print(f"   Branches:")
                        for i, branch_id in enumerate(deps[branch_point], 1):
                            branch_path = [branch_id]
                            current = branch_id
                            while current in deps and deps[current]:
                                next_id = deps[current][0]
                                branch_path.append(next_id)
                                current = next_id
                            print(f"     Branch {i}: {' -> '.join(branch_path)}")
            else:
                print(f"   Workflow Type: LINEAR")
                print(f"   Reference Order: {' -> '.join(eval_system.reference_answer['execution_order'])}")
            
            if eval_system.id_mapping:
                print(f"   ID Obfuscation: {len(eval_system.id_mapping)} steps")
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""You are an AI agent tasked with reconstructing a workflow whose steps have been shuffled.
            **IMPORTANT INSTRUCTIONS:**
            - Step IDs are randomized (e.g., '3f94c550') and DO NOT reflect execution order
            - DO NOT sort by step_id or by name alphabetically
            - Infer the correct execution order based on:
            1. Step types and tool semantics (validation â†’ information â†’ trigger â†’ monitoring â†’ conditional â†’ termination â†’ output)
            2. Data dependencies: {{{{step_X.output}}}} means step X must execute before the current step
            3. Conditional logic: steps may reference {{{{step_Y.condition_result}}}} to determine branching

            **BRANCHING RULES:**
            - If a conditional step has multiple outcome paths, include ALL target steps in its next_steps array
            - Example: A conditional check may lead to both a success termination and a failure termination
            - Both branches should be listed in the conditional step's next_steps: ["success_step_id", "failure_step_id"]

            **OUTPUT FORMAT:**
            Return ONLY a JSON object (no prose, no backticks, no markdown):
            {{"workflow_steps": [{{"step_id": "<id>", "next_steps": ["<id>", ...]}}, ...]}}

            **SHUFFLED WORKFLOW:**
            {json.dumps(shuffled_workflow, indent=2)}

            Return the workflow_steps array in the correct execution order."""

            print("\nğŸ“¤ Sending to LLM...")
            print(f"   Prompt length: {len(prompt)} characters")
            
            try:
                llm_output = await llm_interface.generate_response(prompt)
                # print(f"âœ… Received response ({len(llm_output)} chars)")
                with open(project_output_dir / 'llm_response.txt', 'w', encoding='utf-8') as f:
                    f.write(llm_output)
            except asyncio.TimeoutError:
                raise Exception("LLM request timed out")
            except Exception as e:
                raise Exception(f"LLM request failed: {type(e).__name__}: {e}")
            
            # ä¿å­˜LLMåŸå§‹è¾“å‡º
            with open(project_output_dir / 'llm_raw_output.txt', 'w', encoding='utf-8') as f:
                f.write(llm_output)
            
            # è§£æLLMè¾“å‡º
            try:
                generated_workflow = parse_llm_output(llm_output)
                print("âœ… Successfully parsed LLM output")
                
                with open(project_output_dir / 'generated_workflow.json', 'w', encoding='utf-8') as f:
                    json.dump(generated_workflow, f, indent=2, ensure_ascii=False)
                    
            except Exception as e:
                raise Exception(f"Failed to parse LLM output: {e}")
            
            # è¯„æµ‹
            print("\nğŸ” Evaluating...")
            results = eval_system.evaluate(generated_workflow)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = eval_system.generate_report(
                results,
                output_path=project_output_dir / 'evaluation_report.txt'
            )
            
            print("\n" + report)
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            with open(project_output_dir / 'evaluation_results.json', 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            summary_results['success'] += 1
            summary_results['details'].append({
                'project_num': project_num,
                'status': 'success',
                'results': results
            })
            
            print(f"\nâœ… mcp_project{project_num} evaluation complete!")
            
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ Error processing mcp_project{project_num}: {error_msg}")
            
            # è®°å½•é”™è¯¯ä¿¡æ¯
            with open(project_output_dir / 'error.txt', 'w', encoding='utf-8') as f:
                f.write(f"Error: {error_msg}\n")
                f.write(f"Project path: {project_path}\n")
            
            summary_results['failed'] += 1
            summary_results['details'].append({
                'project_num': project_num,
                'status': 'failed',
                'error': error_msg
            })
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("BATCH EVALUATION SUMMARY")
    print(f"{'='*80}")
    print(f"Total projects: {summary_results['total']}")
    print(f"Success: {summary_results['success']}")
    print(f"Skipped (no workflow.json): {summary_results['skipped']}")
    print(f"Failed: {summary_results['failed']}")
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    with open(output_root_dir / 'batch_summary.json', 'w', encoding='utf-8') as f:
        json.dump(summary_results, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆå¯è¯»çš„æ±‡æ€»æŠ¥å‘Š
    summary_report = generate_summary_report(summary_results)
    with open(output_root_dir / 'batch_summary_report.txt', 'w', encoding='utf-8') as f:
        f.write(summary_report)
    
    print(f"\nâœ… Batch evaluation complete!")
    print(f"   Results saved to: {output_root_dir.absolute()}")


def generate_summary_report(summary_results):
    """ç”Ÿæˆå¯è¯»çš„æ±‡æ€»æŠ¥å‘Š"""
    lines = [
        "=" * 80,
        "BATCH EVALUATION SUMMARY REPORT",
        "=" * 80,
        "",
        f"Total Projects: {summary_results['total']}",
        f"Successful: {summary_results['success']}",
        f"Skipped: {summary_results['skipped']}",
        f"Failed: {summary_results['failed']}",
        "",
        "-" * 80,
        "DETAILED RESULTS",
        "-" * 80,
        ""
    ]
    
    for detail in summary_results['details']:
        project_num = detail['project_num']
        status = detail['status']
        
        if status == 'success':
            results = detail.get('results', {})
            score = results.get('final_score', 'N/A')
            lines.append(f"Project {project_num}: âœ… SUCCESS (Score: {score})")
        elif status == 'skipped':
            reason = detail.get('reason', 'Unknown')
            lines.append(f"Project {project_num}: âš ï¸ SKIPPED ({reason})")
        else:
            error = detail.get('error', 'Unknown error')
            lines.append(f"Project {project_num}: âŒ FAILED ({error[:50]}...)")
    
    lines.append("")
    lines.append("=" * 80)
    
    return "\n".join(lines)

if __name__ == '__main__':
    asyncio.run(main())
